{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸŒ§ï¸ Rainfall Forecasting with LSTM Neural Network\n",
        "\n",
        "## Project Overview\n",
        "This notebook implements a Long Short-Term Memory (LSTM) neural network for 7-day rainfall forecasting using the cleaned rainfall dataset.\n",
        "\n",
        "### Key Features:\n",
        "- **LSTM Architecture**: Optimized for time series forecasting\n",
        "- **Feature Selection**: Focus on most correlated features from actual dataset\n",
        "- **7-Day Forecasting**: Predict rainfall for the next 7 days\n",
        "- **Validation Period**: 2024-2025 data for forecasting evaluation\n",
        "- **Training Period**: 1990-2022 data for model training\n",
        "\n",
        "### Data Structure:\n",
        "- **Source**: `cleaned_rainfall_data.csv`\n",
        "- **Features**: Hourly weather data (SLP, DBT, RH, etc.)\n",
        "- **Target**: Rainfall (RF) in mm\n",
        "- **Time Range**: 1990-2025\n",
        "\n",
        "### Model Configuration:\n",
        "- **Architecture**: LSTM with attention mechanism\n",
        "- **Sequence Length**: 24 hours (1 day) for prediction\n",
        "- **Features**: Most correlated weather variables\n",
        "- **Output**: 7-day rainfall forecast\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ“š Libraries imported successfully!\n",
            "TensorFlow version: 2.10.0\n",
            "GPU available: []\n"
          ]
        }
      ],
      "source": [
        "# Cell 1: Library Imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime, timedelta\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Deep Learning Libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Attention, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.regularizers import l1_l2\n",
        "\n",
        "# Scikit-learn for preprocessing and evaluation\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "print(\"ðŸ“š Libraries imported successfully!\")\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "print(f\"GPU available: {tf.config.list_physical_devices('GPU')}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ“Š Loading rainfall dataset...\n",
            "ðŸ“ˆ Dataset loaded successfully!\n",
            "   Shape: (68748, 37)\n",
            "   Date range: 1990-01-01 00:00:00 to 2025-03-02 12:00:00\n",
            "   Total records: 68,748\n",
            "\n",
            "ðŸ“Š Rainfall Statistics:\n",
            "   Mean: 1.91 mm\n",
            "   Median: 0.00 mm\n",
            "   Max: 216.20 mm\n",
            "   Non-zero records: 13,177 (19.2%)\n",
            "\n",
            "âš ï¸ Missing values:\n",
            "H    68748\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Cell 2: Data Loading and Exploration\n",
        "def load_and_explore_data(csv_file='cleaned_rainfall_data.csv'):\n",
        "    \"\"\"\n",
        "    Load and explore the rainfall dataset\n",
        "    \"\"\"\n",
        "    print(\"ðŸ“Š Loading rainfall dataset...\")\n",
        "    \n",
        "    # Load the dataset\n",
        "    df = pd.read_csv(csv_file)\n",
        "    \n",
        "    # Create datetime column\n",
        "    df['datetime'] = pd.to_datetime(df[['YEAR', 'MN', 'DT', 'HR']].rename(columns={\n",
        "        'YEAR': 'year', 'MN': 'month', 'DT': 'day', 'HR': 'hour'\n",
        "    }))\n",
        "    \n",
        "    # Sort by datetime\n",
        "    df = df.sort_values('datetime').reset_index(drop=True)\n",
        "    \n",
        "    print(f\"ðŸ“ˆ Dataset loaded successfully!\")\n",
        "    print(f\"   Shape: {df.shape}\")\n",
        "    print(f\"   Date range: {df['datetime'].min()} to {df['datetime'].max()}\")\n",
        "    print(f\"   Total records: {len(df):,}\")\n",
        "    \n",
        "    # Basic statistics\n",
        "    print(f\"\\nðŸ“Š Rainfall Statistics:\")\n",
        "    print(f\"   Mean: {df['RF'].mean():.2f} mm\")\n",
        "    print(f\"   Median: {df['RF'].median():.2f} mm\")\n",
        "    print(f\"   Max: {df['RF'].max():.2f} mm\")\n",
        "    print(f\"   Non-zero records: {(df['RF'] > 0).sum():,} ({(df['RF'] > 0).mean()*100:.1f}%)\")\n",
        "    \n",
        "    # Missing values\n",
        "    missing = df.isnull().sum()\n",
        "    if missing.sum() > 0:\n",
        "        print(f\"\\nâš ï¸ Missing values:\")\n",
        "        print(missing[missing > 0])\n",
        "    else:\n",
        "        print(f\"\\nâœ… No missing values found!\")\n",
        "    \n",
        "    return df\n",
        "\n",
        "# Load the data\n",
        "df = load_and_explore_data()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ” Analyzing feature correlations with rainfall...\n",
            "\n",
            "ðŸ“Š Top 15 Most Correlated Features:\n",
            "    1. A   : 0.255\n",
            "    2. TC  : 0.253\n",
            "    3. RH  : 0.243\n",
            "    4. h   : 0.195\n",
            "    5. Cl  : 0.176\n",
            "    6. A.1 : 0.170\n",
            "    7. a   : 0.163\n",
            "    8. Ht  : 0.156\n",
            "    9. Dm  : 0.141\n",
            "   10. Dl  : 0.135\n",
            "   11. DPT : 0.124\n",
            "   12. c   : 0.123\n",
            "   13. VP  : 0.117\n",
            "   14. VV  : 0.096\n",
            "   15. WBT : 0.072\n",
            "\n",
            "âœ… Selected 17 features with correlation >= 0.05:\n",
            "   Features: ['A', 'TC', 'RH', 'h', 'Cl', 'A.1', 'a', 'Ht', 'Dm', 'Dl', 'DPT', 'c', 'VP', 'VV', 'WBT', 'Cm', 'A.2']\n",
            "\n",
            "ðŸ“‹ Final feature matrix shape: (68748, 19)\n",
            "   Features: 18 weather variables + datetime\n"
          ]
        }
      ],
      "source": [
        "# Cell 3: Feature Selection and Correlation Analysis\n",
        "def select_correlated_features(df, target_col='RF', min_correlation=0.1):\n",
        "    \"\"\"\n",
        "    Select the most correlated features with rainfall\n",
        "    \"\"\"\n",
        "    print(\"ðŸ” Analyzing feature correlations with rainfall...\")\n",
        "    \n",
        "    # Define weather variables to analyze\n",
        "    weather_vars = [\n",
        "        'SLP', 'MSLP', 'DBT', 'WBT', 'DPT', 'RH', 'VP',\n",
        "        'DD', 'FFF', 'AW', 'VV', 'Cl', 'A', 'Cm', 'A.1',\n",
        "        'Ch', 'A.2', 'Dl', 'Dm', 'Dh', 'TC', 'h', 'c',\n",
        "        'a', 'Ht', 'EVP', 'DW', 'P', 'H', 'WAT'\n",
        "    ]\n",
        "    \n",
        "    # Calculate correlations with rainfall\n",
        "    correlations = {}\n",
        "    for var in weather_vars:\n",
        "        if var in df.columns:\n",
        "            corr = df[var].corr(df[target_col])\n",
        "            correlations[var] = abs(corr)  # Use absolute correlation\n",
        "    \n",
        "    # Sort by correlation strength\n",
        "    sorted_correlations = sorted(correlations.items(), key=lambda x: x[1], reverse=True)\n",
        "    \n",
        "    print(f\"\\nðŸ“Š Top 15 Most Correlated Features:\")\n",
        "    for i, (var, corr) in enumerate(sorted_correlations[:15]):\n",
        "        print(f\"   {i+1:2d}. {var:4s}: {corr:.3f}\")\n",
        "    \n",
        "    # Select features above minimum correlation threshold\n",
        "    selected_features = [var for var, corr in sorted_correlations if corr >= min_correlation]\n",
        "    \n",
        "    print(f\"\\nâœ… Selected {len(selected_features)} features with correlation >= {min_correlation}:\")\n",
        "    print(f\"   Features: {selected_features}\")\n",
        "    \n",
        "    return selected_features, sorted_correlations\n",
        "\n",
        "# Select correlated features\n",
        "selected_features, correlations = select_correlated_features(df, min_correlation=0.05)\n",
        "\n",
        "# Create feature matrix\n",
        "feature_cols = selected_features + ['RF']  # Include target for lag features\n",
        "df_features = df[['datetime'] + feature_cols].copy()\n",
        "\n",
        "print(f\"\\nðŸ“‹ Final feature matrix shape: {df_features.shape}\")\n",
        "print(f\"   Features: {len(feature_cols)} weather variables + datetime\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ”„ Creating time series features with sequence length 24...\n",
            "âœ… Time series features created!\n",
            "   Final shape: (68698, 50)\n",
            "   Features: 50 total features\n",
            "\n",
            "ðŸ“‹ Feature columns:\n",
            "   Weather variables: ['A', 'TC', 'RH', 'h', 'Cl', 'A.1', 'a', 'Ht', 'Dm', 'Dl', 'DPT', 'c', 'VP', 'VV', 'WBT', 'Cm', 'A.2']\n",
            "   Temporal features: hour, month, day_of_year, etc.\n",
            "   Lag features: RF_lag_1 to RF_lag_24\n",
            "   Rolling features: RF_rolling_mean/std/max for 3,6,12,24h windows\n"
          ]
        }
      ],
      "source": [
        "# Cell 4: Create Time Series Features\n",
        "def create_time_series_features(df, feature_cols, sequence_length=24):\n",
        "    \"\"\"\n",
        "    Create time series features for LSTM model\n",
        "    \"\"\"\n",
        "    print(f\"ðŸ”„ Creating time series features with sequence length {sequence_length}...\")\n",
        "    \n",
        "    # Add temporal features\n",
        "    df['hour'] = df['datetime'].dt.hour\n",
        "    df['day_of_year'] = df['datetime'].dt.dayofyear\n",
        "    df['month'] = df['datetime'].dt.month\n",
        "    df['day_of_week'] = df['datetime'].dt.dayofweek\n",
        "    df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
        "    \n",
        "    # Cyclical encoding for temporal features\n",
        "    df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
        "    df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
        "    df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
        "    df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
        "    df['day_of_year_sin'] = np.sin(2 * np.pi * df['day_of_year'] / 365)\n",
        "    df['day_of_year_cos'] = np.cos(2 * np.pi * df['day_of_year'] / 365)\n",
        "    \n",
        "    # Add lag features for rainfall (most important for prediction)\n",
        "    for lag in [1, 2, 3, 6, 12, 24]:\n",
        "        df[f'RF_lag_{lag}'] = df['RF'].shift(lag)\n",
        "    \n",
        "    # Add rolling statistics for rainfall\n",
        "    for window in [3, 6, 12, 24]:\n",
        "        df[f'RF_rolling_mean_{window}'] = df['RF'].rolling(window=window).mean()\n",
        "        df[f'RF_rolling_std_{window}'] = df['RF'].rolling(window=window).std()\n",
        "        df[f'RF_rolling_max_{window}'] = df['RF'].rolling(window=window).max()\n",
        "    \n",
        "    # Add rainfall indicators\n",
        "    df['is_rainy'] = (df['RF'] > 0).astype(int)\n",
        "    df['is_heavy_rain'] = (df['RF'] > 10).astype(int)\n",
        "    \n",
        "    # Fill NaN values\n",
        "    df = df.fillna(0)\n",
        "    \n",
        "    # Remove initial rows with NaN values from lag features\n",
        "    df = df.iloc[50:].reset_index(drop=True)\n",
        "    \n",
        "    print(f\"âœ… Time series features created!\")\n",
        "    print(f\"   Final shape: {df.shape}\")\n",
        "    print(f\"   Features: {len(df.columns)} total features\")\n",
        "    \n",
        "    return df\n",
        "\n",
        "# Create time series features\n",
        "df_ts = create_time_series_features(df_features, selected_features)\n",
        "\n",
        "# Display feature information\n",
        "print(f\"\\nðŸ“‹ Feature columns:\")\n",
        "print(f\"   Weather variables: {selected_features}\")\n",
        "print(f\"   Temporal features: hour, month, day_of_year, etc.\")\n",
        "print(f\"   Lag features: RF_lag_1 to RF_lag_24\")\n",
        "print(f\"   Rolling features: RF_rolling_mean/std/max for 3,6,12,24h windows\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ”„ Preparing LSTM data with sequence length 24...\n",
            "   Features: 48 columns\n",
            "   Target: RF\n",
            "\n",
            "ðŸ“Š Data split by year:\n",
            "   Training (1990-2022): 65,458 samples\n",
            "   Test (2023): 2,156 samples\n",
            "   Validation (2024-2025): 1,084 samples\n",
            "âœ… Data scaling completed!\n",
            "   Feature scaler: MinMaxScaler\n",
            "   Target scaler: MinMaxScaler\n"
          ]
        }
      ],
      "source": [
        "# Cell 5: Data Preparation and Splitting\n",
        "def prepare_lstm_data(df, sequence_length=24, forecast_horizon=7):\n",
        "    \"\"\"\n",
        "    Prepare data for LSTM model with proper time series splitting\n",
        "    \"\"\"\n",
        "    print(f\"ðŸ”„ Preparing LSTM data with sequence length {sequence_length}...\")\n",
        "    \n",
        "    # Define feature columns (exclude datetime and target)\n",
        "    feature_cols = [col for col in df.columns if col not in ['datetime', 'RF']]\n",
        "    target_col = 'RF'\n",
        "    \n",
        "    print(f\"   Features: {len(feature_cols)} columns\")\n",
        "    print(f\"   Target: {target_col}\")\n",
        "    \n",
        "    # Split data by year (chronological split)\n",
        "    train_data = df[df['datetime'].dt.year <= 2022].copy()\n",
        "    test_data = df[df['datetime'].dt.year == 2023].copy()\n",
        "    validation_data = df[df['datetime'].dt.year >= 2024].copy()\n",
        "    \n",
        "    print(f\"\\nðŸ“Š Data split by year:\")\n",
        "    print(f\"   Training (1990-2022): {len(train_data):,} samples\")\n",
        "    print(f\"   Test (2023): {len(test_data):,} samples\")\n",
        "    print(f\"   Validation (2024-2025): {len(validation_data):,} samples\")\n",
        "    \n",
        "    # Scale features\n",
        "    scaler = MinMaxScaler()\n",
        "    train_scaled = scaler.fit_transform(train_data[feature_cols])\n",
        "    test_scaled = scaler.transform(test_data[feature_cols])\n",
        "    validation_scaled = scaler.transform(validation_data[feature_cols])\n",
        "    \n",
        "    # Scale target (rainfall)\n",
        "    target_scaler = MinMaxScaler()\n",
        "    train_target_scaled = target_scaler.fit_transform(train_data[[target_col]])\n",
        "    test_target_scaled = target_scaler.transform(test_data[[target_col]])\n",
        "    validation_target_scaled = target_scaler.transform(validation_data[[target_col]])\n",
        "    \n",
        "    print(f\"âœ… Data scaling completed!\")\n",
        "    print(f\"   Feature scaler: MinMaxScaler\")\n",
        "    print(f\"   Target scaler: MinMaxScaler\")\n",
        "    \n",
        "    return {\n",
        "        'train_data': train_data,\n",
        "        'test_data': test_data,\n",
        "        'validation_data': validation_data,\n",
        "        'train_scaled': train_scaled,\n",
        "        'test_scaled': test_scaled,\n",
        "        'validation_scaled': validation_scaled,\n",
        "        'train_target_scaled': train_target_scaled,\n",
        "        'test_target_scaled': test_target_scaled,\n",
        "        'validation_target_scaled': validation_target_scaled,\n",
        "        'feature_cols': feature_cols,\n",
        "        'target_col': target_col,\n",
        "        'scaler': scaler,\n",
        "        'target_scaler': target_scaler,\n",
        "        'sequence_length': sequence_length,\n",
        "        'forecast_horizon': forecast_horizon\n",
        "    }\n",
        "\n",
        "# Prepare data for LSTM\n",
        "data_dict = prepare_lstm_data(df_ts, sequence_length=24, forecast_horizon=7)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ”„ Creating LSTM sequences...\n",
            "âœ… LSTM sequences created!\n",
            "   Training sequences: (65428, 24, 48)\n",
            "   Test sequences: (2126, 24, 48)\n",
            "   Validation sequences: (1054, 24, 48)\n",
            "   Sequence length: 24 hours\n",
            "   Forecast horizon: 7 days\n"
          ]
        }
      ],
      "source": [
        "# Cell 6: Create LSTM Sequences\n",
        "def create_sequences(data, target, sequence_length, forecast_horizon=1):\n",
        "    \"\"\"\n",
        "    Create sequences for LSTM model\n",
        "    \"\"\"\n",
        "    X, y = [], []\n",
        "    \n",
        "    for i in range(sequence_length, len(data) - forecast_horizon + 1):\n",
        "        # Input sequence\n",
        "        X.append(data[i-sequence_length:i])\n",
        "        # Target (single value or sequence)\n",
        "        if forecast_horizon == 1:\n",
        "            y.append(target[i])\n",
        "        else:\n",
        "            y.append(target[i:i+forecast_horizon])\n",
        "    \n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "def prepare_lstm_sequences(data_dict):\n",
        "    \"\"\"\n",
        "    Prepare sequences for LSTM training\n",
        "    \"\"\"\n",
        "    print(\"ðŸ”„ Creating LSTM sequences...\")\n",
        "    \n",
        "    sequence_length = data_dict['sequence_length']\n",
        "    forecast_horizon = data_dict['forecast_horizon']\n",
        "    \n",
        "    # Create training sequences\n",
        "    X_train, y_train = create_sequences(\n",
        "        data_dict['train_scaled'], \n",
        "        data_dict['train_target_scaled'].flatten(),\n",
        "        sequence_length, \n",
        "        forecast_horizon\n",
        "    )\n",
        "    \n",
        "    # Create test sequences\n",
        "    X_test, y_test = create_sequences(\n",
        "        data_dict['test_scaled'], \n",
        "        data_dict['test_target_scaled'].flatten(),\n",
        "        sequence_length, \n",
        "        forecast_horizon\n",
        "    )\n",
        "    \n",
        "    # Create validation sequences\n",
        "    X_val, y_val = create_sequences(\n",
        "        data_dict['validation_scaled'], \n",
        "        data_dict['validation_target_scaled'].flatten(),\n",
        "        sequence_length, \n",
        "        forecast_horizon\n",
        "    )\n",
        "    \n",
        "    print(f\"âœ… LSTM sequences created!\")\n",
        "    print(f\"   Training sequences: {X_train.shape}\")\n",
        "    print(f\"   Test sequences: {X_test.shape}\")\n",
        "    print(f\"   Validation sequences: {X_val.shape}\")\n",
        "    print(f\"   Sequence length: {sequence_length} hours\")\n",
        "    print(f\"   Forecast horizon: {forecast_horizon} days\")\n",
        "    \n",
        "    return X_train, y_train, X_test, y_test, X_val, y_val\n",
        "\n",
        "# Create LSTM sequences\n",
        "X_train, y_train, X_test, y_test, X_val, y_val = prepare_lstm_sequences(data_dict)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ—ï¸ Building LSTM model...\n",
            "âœ… LSTM model built!\n",
            "   Input shape: (24, 48)\n",
            "   Output shape: 7 days\n",
            "   Total parameters: 156,871\n",
            "\n",
            "ðŸ“‹ Model Architecture:\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm (LSTM)                 (None, 24, 128)           90624     \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 24, 64)            49408     \n",
            "                                                                 \n",
            " lstm_2 (LSTM)               (None, 32)                12416     \n",
            "                                                                 \n",
            " dense (Dense)               (None, 64)                2112      \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 64)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 32)                2080      \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 32)                0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 7)                 231       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 156,871\n",
            "Trainable params: 156,871\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Cell 7: Build LSTM Model\n",
        "def build_lstm_model(input_shape, forecast_horizon=7):\n",
        "    \"\"\"\n",
        "    Build LSTM model for rainfall forecasting\n",
        "    \"\"\"\n",
        "    print(\"ðŸ—ï¸ Building LSTM model...\")\n",
        "    \n",
        "    model = Sequential([\n",
        "        # First LSTM layer\n",
        "        LSTM(128, return_sequences=True, input_shape=input_shape,\n",
        "             dropout=0.2, recurrent_dropout=0.2),\n",
        "        \n",
        "        # Second LSTM layer\n",
        "        LSTM(64, return_sequences=True,\n",
        "             dropout=0.2, recurrent_dropout=0.2),\n",
        "        \n",
        "        # Third LSTM layer\n",
        "        LSTM(32, return_sequences=False,\n",
        "             dropout=0.2, recurrent_dropout=0.2),\n",
        "        \n",
        "        # Dense layers\n",
        "        Dense(64, activation='relu', kernel_regularizer=l1_l2(0.01, 0.01)),\n",
        "        Dropout(0.3),\n",
        "        \n",
        "        Dense(32, activation='relu', kernel_regularizer=l1_l2(0.01, 0.01)),\n",
        "        Dropout(0.3),\n",
        "        \n",
        "        # Output layer\n",
        "        Dense(forecast_horizon, activation='linear')\n",
        "    ])\n",
        "    \n",
        "    # Compile model\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=0.001),\n",
        "        loss='mse',\n",
        "        metrics=['mae', 'mse']\n",
        "    )\n",
        "    \n",
        "    print(f\"âœ… LSTM model built!\")\n",
        "    print(f\"   Input shape: {input_shape}\")\n",
        "    print(f\"   Output shape: {forecast_horizon} days\")\n",
        "    print(f\"   Total parameters: {model.count_params():,}\")\n",
        "    \n",
        "    return model\n",
        "\n",
        "# Build the model\n",
        "input_shape = (X_train.shape[1], X_train.shape[2])  # (sequence_length, n_features)\n",
        "model = build_lstm_model(input_shape, forecast_horizon=7)\n",
        "\n",
        "# Display model architecture\n",
        "print(f\"\\nðŸ“‹ Model Architecture:\")\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸš€ Training LSTM model...\n",
            "Epoch 1/100\n",
            "2045/2045 [==============================] - 205s 96ms/step - loss: 0.2524 - mae: 0.0151 - mse: 0.0013 - val_loss: 0.0066 - val_mae: 0.0158 - val_mse: 0.0014 - lr: 0.0010\n",
            "Epoch 2/100\n",
            "2045/2045 [==============================] - 241s 118ms/step - loss: 0.0066 - mae: 0.0150 - mse: 0.0013 - val_loss: 0.0066 - val_mae: 0.0159 - val_mse: 0.0014 - lr: 0.0010\n",
            "Epoch 3/100\n",
            "2045/2045 [==============================] - 274s 134ms/step - loss: 0.0066 - mae: 0.0150 - mse: 0.0013 - val_loss: 0.0067 - val_mae: 0.0155 - val_mse: 0.0014 - lr: 0.0010\n",
            "Epoch 4/100\n",
            "2045/2045 [==============================] - 288s 141ms/step - loss: 0.0066 - mae: 0.0150 - mse: 0.0013 - val_loss: 0.0067 - val_mae: 0.0165 - val_mse: 0.0014 - lr: 0.0010\n",
            "Epoch 5/100\n",
            "2045/2045 [==============================] - 267s 130ms/step - loss: 0.0066 - mae: 0.0150 - mse: 0.0013 - val_loss: 0.0067 - val_mae: 0.0161 - val_mse: 0.0014 - lr: 0.0010\n",
            "Epoch 6/100\n",
            "2045/2045 [==============================] - 321s 157ms/step - loss: 0.0066 - mae: 0.0150 - mse: 0.0013 - val_loss: 0.0066 - val_mae: 0.0148 - val_mse: 0.0014 - lr: 0.0010\n",
            "Epoch 7/100\n",
            "2045/2045 [==============================] - 285s 140ms/step - loss: 0.0066 - mae: 0.0150 - mse: 0.0013 - val_loss: 0.0067 - val_mae: 0.0158 - val_mse: 0.0014 - lr: 0.0010\n",
            "Epoch 8/100\n",
            "2045/2045 [==============================] - 268s 131ms/step - loss: 0.0066 - mae: 0.0150 - mse: 0.0013 - val_loss: 0.0067 - val_mae: 0.0148 - val_mse: 0.0014 - lr: 0.0010\n",
            "Epoch 9/100\n",
            "2045/2045 [==============================] - 234s 115ms/step - loss: 0.0066 - mae: 0.0150 - mse: 0.0013 - val_loss: 0.0067 - val_mae: 0.0147 - val_mse: 0.0014 - lr: 0.0010\n",
            "Epoch 10/100\n",
            "2045/2045 [==============================] - 250s 122ms/step - loss: 0.0066 - mae: 0.0150 - mse: 0.0013 - val_loss: 0.0067 - val_mae: 0.0168 - val_mse: 0.0014 - lr: 0.0010\n",
            "Epoch 11/100\n",
            "2045/2045 [==============================] - ETA: 0s - loss: 0.0066 - mae: 0.0150 - mse: 0.0013\n",
            "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "2045/2045 [==============================] - 243s 119ms/step - loss: 0.0066 - mae: 0.0150 - mse: 0.0013 - val_loss: 0.0066 - val_mae: 0.0157 - val_mse: 0.0014 - lr: 0.0010\n",
            "Epoch 12/100\n",
            "2045/2045 [==============================] - 251s 123ms/step - loss: 0.0038 - mae: 0.0150 - mse: 0.0013 - val_loss: 0.0041 - val_mae: 0.0155 - val_mse: 0.0014 - lr: 5.0000e-04\n",
            "Epoch 13/100\n",
            "2045/2045 [==============================] - 251s 123ms/step - loss: 0.0037 - mae: 0.0150 - mse: 0.0013 - val_loss: 0.0036 - val_mae: 0.0153 - val_mse: 0.0014 - lr: 5.0000e-04\n",
            "Epoch 14/100\n",
            "2045/2045 [==============================] - 273s 133ms/step - loss: 0.0038 - mae: 0.0150 - mse: 0.0013 - val_loss: 0.0040 - val_mae: 0.0164 - val_mse: 0.0014 - lr: 5.0000e-04\n",
            "Epoch 15/100\n",
            "2045/2045 [==============================] - 290s 142ms/step - loss: 0.0038 - mae: 0.0150 - mse: 0.0013 - val_loss: 0.0040 - val_mae: 0.0148 - val_mse: 0.0014 - lr: 5.0000e-04\n",
            "Epoch 16/100\n",
            "2045/2045 [==============================] - 191s 94ms/step - loss: 0.0038 - mae: 0.0150 - mse: 0.0013 - val_loss: 0.0039 - val_mae: 0.0154 - val_mse: 0.0014 - lr: 5.0000e-04\n",
            "Epoch 17/100\n",
            "2045/2045 [==============================] - 183s 90ms/step - loss: 0.0038 - mae: 0.0150 - mse: 0.0013 - val_loss: 0.0040 - val_mae: 0.0147 - val_mse: 0.0014 - lr: 5.0000e-04\n",
            "Epoch 18/100\n",
            "2045/2045 [==============================] - 191s 93ms/step - loss: 0.0038 - mae: 0.0150 - mse: 0.0013 - val_loss: 0.0040 - val_mae: 0.0153 - val_mse: 0.0014 - lr: 5.0000e-04\n",
            "Epoch 19/100\n",
            "2045/2045 [==============================] - 220s 107ms/step - loss: 0.0038 - mae: 0.0150 - mse: 0.0013 - val_loss: 0.0041 - val_mae: 0.0152 - val_mse: 0.0014 - lr: 5.0000e-04\n",
            "Epoch 20/100\n",
            "2045/2045 [==============================] - 270s 132ms/step - loss: 0.0038 - mae: 0.0150 - mse: 0.0013 - val_loss: 0.0038 - val_mae: 0.0155 - val_mse: 0.0014 - lr: 5.0000e-04\n",
            "Epoch 21/100\n",
            "2045/2045 [==============================] - 310s 152ms/step - loss: 0.0038 - mae: 0.0150 - mse: 0.0013 - val_loss: 0.0041 - val_mae: 0.0157 - val_mse: 0.0014 - lr: 5.0000e-04\n",
            "Epoch 22/100\n",
            "2045/2045 [==============================] - 333s 163ms/step - loss: 0.0038 - mae: 0.0150 - mse: 0.0013 - val_loss: 0.0041 - val_mae: 0.0156 - val_mse: 0.0014 - lr: 5.0000e-04\n",
            "Epoch 23/100\n",
            "2045/2045 [==============================] - ETA: 0s - loss: 0.0038 - mae: 0.0150 - mse: 0.0013\n",
            "Epoch 23: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "2045/2045 [==============================] - 353s 173ms/step - loss: 0.0038 - mae: 0.0150 - mse: 0.0013 - val_loss: 0.0038 - val_mae: 0.0159 - val_mse: 0.0014 - lr: 5.0000e-04\n",
            "Epoch 24/100\n",
            "2045/2045 [==============================] - 342s 167ms/step - loss: 0.0026 - mae: 0.0150 - mse: 0.0013 - val_loss: 0.0027 - val_mae: 0.0159 - val_mse: 0.0014 - lr: 2.5000e-04\n",
            "Epoch 25/100\n",
            "2045/2045 [==============================] - 319s 156ms/step - loss: 0.0026 - mae: 0.0150 - mse: 0.0013 - val_loss: 0.0025 - val_mae: 0.0156 - val_mse: 0.0014 - lr: 2.5000e-04\n",
            "Epoch 26/100\n",
            "2045/2045 [==============================] - 310s 152ms/step - loss: 0.0026 - mae: 0.0150 - mse: 0.0013 - val_loss: 0.0024 - val_mae: 0.0164 - val_mse: 0.0014 - lr: 2.5000e-04\n",
            "Epoch 27/100\n",
            "2045/2045 [==============================] - 271s 133ms/step - loss: 0.0026 - mae: 0.0150 - mse: 0.0013 - val_loss: 0.0029 - val_mae: 0.0154 - val_mse: 0.0014 - lr: 2.5000e-04\n",
            "Epoch 28/100\n",
            "2045/2045 [==============================] - 316s 154ms/step - loss: 0.0026 - mae: 0.0150 - mse: 0.0013 - val_loss: 0.0027 - val_mae: 0.0161 - val_mse: 0.0014 - lr: 2.5000e-04\n",
            "Epoch 29/100\n",
            "2045/2045 [==============================] - 243s 119ms/step - loss: 0.0026 - mae: 0.0150 - mse: 0.0013 - val_loss: 0.0026 - val_mae: 0.0161 - val_mse: 0.0014 - lr: 2.5000e-04\n",
            "Epoch 30/100\n",
            "2045/2045 [==============================] - 352s 172ms/step - loss: 0.0026 - mae: 0.0150 - mse: 0.0013 - val_loss: 0.0027 - val_mae: 0.0157 - val_mse: 0.0014 - lr: 2.5000e-04\n",
            "Epoch 31/100\n",
            "2045/2045 [==============================] - 351s 172ms/step - loss: 0.0026 - mae: 0.0150 - mse: 0.0013 - val_loss: 0.0027 - val_mae: 0.0159 - val_mse: 0.0014 - lr: 2.5000e-04\n",
            "Epoch 32/100\n",
            "2045/2045 [==============================] - 294s 144ms/step - loss: 0.0026 - mae: 0.0150 - mse: 0.0013 - val_loss: 0.0028 - val_mae: 0.0153 - val_mse: 0.0014 - lr: 2.5000e-04\n",
            "Epoch 33/100\n",
            "2045/2045 [==============================] - 251s 123ms/step - loss: 0.0026 - mae: 0.0150 - mse: 0.0013 - val_loss: 0.0027 - val_mae: 0.0156 - val_mse: 0.0014 - lr: 2.5000e-04\n",
            "Epoch 34/100\n",
            "2045/2045 [==============================] - 280s 137ms/step - loss: 0.0026 - mae: 0.0150 - mse: 0.0013 - val_loss: 0.0028 - val_mae: 0.0166 - val_mse: 0.0014 - lr: 2.5000e-04\n",
            "Epoch 35/100\n",
            "2045/2045 [==============================] - ETA: 0s - loss: 0.0026 - mae: 0.0150 - mse: 0.0013\n",
            "Epoch 35: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "2045/2045 [==============================] - 343s 168ms/step - loss: 0.0026 - mae: 0.0150 - mse: 0.0013 - val_loss: 0.0026 - val_mae: 0.0167 - val_mse: 0.0014 - lr: 2.5000e-04\n",
            "Epoch 36/100\n",
            "2045/2045 [==============================] - 340s 166ms/step - loss: 0.0019 - mae: 0.0151 - mse: 0.0013 - val_loss: 0.0021 - val_mae: 0.0156 - val_mse: 0.0014 - lr: 1.2500e-04\n",
            "Epoch 37/100\n",
            "2045/2045 [==============================] - 332s 162ms/step - loss: 0.0019 - mae: 0.0150 - mse: 0.0013 - val_loss: 0.0020 - val_mae: 0.0161 - val_mse: 0.0014 - lr: 1.2500e-04\n",
            "Epoch 38/100\n",
            "2045/2045 [==============================] - 324s 158ms/step - loss: 0.0019 - mae: 0.0150 - mse: 0.0013 - val_loss: 0.0018 - val_mae: 0.0157 - val_mse: 0.0014 - lr: 1.2500e-04\n",
            "Epoch 39/100\n",
            "2045/2045 [==============================] - 333s 163ms/step - loss: 0.0019 - mae: 0.0150 - mse: 0.0013 - val_loss: 0.0020 - val_mae: 0.0158 - val_mse: 0.0014 - lr: 1.2500e-04\n",
            "Epoch 40/100\n",
            "2045/2045 [==============================] - 315s 154ms/step - loss: 0.0019 - mae: 0.0150 - mse: 0.0013 - val_loss: 0.0020 - val_mae: 0.0156 - val_mse: 0.0014 - lr: 1.2500e-04\n",
            "Epoch 41/100\n",
            "2045/2045 [==============================] - 227s 111ms/step - loss: 0.0019 - mae: 0.0150 - mse: 0.0013 - val_loss: 0.0019 - val_mae: 0.0157 - val_mse: 0.0014 - lr: 1.2500e-04\n",
            "Epoch 42/100\n",
            "2045/2045 [==============================] - 258s 126ms/step - loss: 0.0019 - mae: 0.0150 - mse: 0.0013 - val_loss: 0.0020 - val_mae: 0.0158 - val_mse: 0.0014 - lr: 1.2500e-04\n",
            "Epoch 43/100\n",
            "2045/2045 [==============================] - 291s 143ms/step - loss: 0.0019 - mae: 0.0150 - mse: 0.0013 - val_loss: 0.0021 - val_mae: 0.0155 - val_mse: 0.0014 - lr: 1.2500e-04\n",
            "Epoch 44/100\n",
            "2045/2045 [==============================] - 373s 182ms/step - loss: 0.0019 - mae: 0.0150 - mse: 0.0013 - val_loss: 0.0020 - val_mae: 0.0158 - val_mse: 0.0014 - lr: 1.2500e-04\n",
            "Epoch 45/100\n",
            "2045/2045 [==============================] - 369s 181ms/step - loss: 0.0019 - mae: 0.0150 - mse: 0.0013 - val_loss: 0.0021 - val_mae: 0.0158 - val_mse: 0.0014 - lr: 1.2500e-04\n",
            "Epoch 46/100\n",
            "2045/2045 [==============================] - 309s 151ms/step - loss: 0.0019 - mae: 0.0150 - mse: 0.0013 - val_loss: 0.0020 - val_mae: 0.0159 - val_mse: 0.0014 - lr: 1.2500e-04\n",
            "Epoch 47/100\n",
            "2045/2045 [==============================] - 374s 183ms/step - loss: 0.0019 - mae: 0.0150 - mse: 0.0013 - val_loss: 0.0020 - val_mae: 0.0156 - val_mse: 0.0014 - lr: 1.2500e-04\n",
            "Epoch 48/100\n",
            "2045/2045 [==============================] - ETA: 0s - loss: 0.0019 - mae: 0.0150 - mse: 0.0013\n",
            "Epoch 48: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "2045/2045 [==============================] - 324s 158ms/step - loss: 0.0019 - mae: 0.0150 - mse: 0.0013 - val_loss: 0.0021 - val_mae: 0.0155 - val_mse: 0.0014 - lr: 1.2500e-04\n",
            "Epoch 49/100\n",
            "2045/2045 [==============================] - 315s 154ms/step - loss: 0.0016 - mae: 0.0150 - mse: 0.0013 - val_loss: 0.0018 - val_mae: 0.0158 - val_mse: 0.0014 - lr: 6.2500e-05\n",
            "Epoch 50/100\n",
            "2045/2045 [==============================] - 305s 149ms/step - loss: 0.0016 - mae: 0.0150 - mse: 0.0013 - val_loss: 0.0018 - val_mae: 0.0157 - val_mse: 0.0014 - lr: 6.2500e-05\n",
            "Epoch 51/100\n",
            "2045/2045 [==============================] - 308s 151ms/step - loss: 0.0016 - mae: 0.0150 - mse: 0.0013 - val_loss: 0.0017 - val_mae: 0.0157 - val_mse: 0.0014 - lr: 6.2500e-05\n",
            "Epoch 52/100\n",
            "2045/2045 [==============================] - 322s 158ms/step - loss: 0.0016 - mae: 0.0150 - mse: 0.0013 - val_loss: 0.0016 - val_mae: 0.0157 - val_mse: 0.0014 - lr: 6.2500e-05\n",
            "Epoch 53/100\n",
            "2045/2045 [==============================] - 283s 139ms/step - loss: 0.0016 - mae: 0.0150 - mse: 0.0013 - val_loss: 0.0017 - val_mae: 0.0156 - val_mse: 0.0014 - lr: 6.2500e-05\n",
            "Epoch 54/100\n",
            "2045/2045 [==============================] - 311s 152ms/step - loss: 0.0016 - mae: 0.0150 - mse: 0.0013 - val_loss: 0.0017 - val_mae: 0.0160 - val_mse: 0.0014 - lr: 6.2500e-05\n",
            "Epoch 55/100\n",
            "2045/2045 [==============================] - 380s 186ms/step - loss: 0.0016 - mae: 0.0150 - mse: 0.0013 - val_loss: 0.0018 - val_mae: 0.0157 - val_mse: 0.0014 - lr: 6.2500e-05\n",
            "Epoch 56/100\n",
            "2045/2045 [==============================] - 411s 201ms/step - loss: 0.0016 - mae: 0.0150 - mse: 0.0013 - val_loss: 0.0017 - val_mae: 0.0159 - val_mse: 0.0014 - lr: 6.2500e-05\n",
            "Epoch 57/100\n",
            "2045/2045 [==============================] - 420s 205ms/step - loss: 0.0016 - mae: 0.0150 - mse: 0.0013 - val_loss: 0.0017 - val_mae: 0.0159 - val_mse: 0.0014 - lr: 6.2500e-05\n",
            "Epoch 58/100\n",
            "2045/2045 [==============================] - 405s 198ms/step - loss: 0.0016 - mae: 0.0150 - mse: 0.0013 - val_loss: 0.0017 - val_mae: 0.0159 - val_mse: 0.0014 - lr: 6.2500e-05\n",
            "Epoch 59/100\n",
            "2045/2045 [==============================] - 434s 212ms/step - loss: 0.0016 - mae: 0.0150 - mse: 0.0013 - val_loss: 0.0017 - val_mae: 0.0157 - val_mse: 0.0014 - lr: 6.2500e-05\n",
            "Epoch 60/100\n",
            "2045/2045 [==============================] - 406s 198ms/step - loss: 0.0016 - mae: 0.0150 - mse: 0.0013 - val_loss: 0.0017 - val_mae: 0.0163 - val_mse: 0.0014 - lr: 6.2500e-05\n",
            "Epoch 61/100\n",
            "2045/2045 [==============================] - ETA: 0s - loss: 0.0016 - mae: 0.0150 - mse: 0.0013\n",
            "Epoch 61: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "2045/2045 [==============================] - 409s 200ms/step - loss: 0.0016 - mae: 0.0150 - mse: 0.0013 - val_loss: 0.0018 - val_mae: 0.0160 - val_mse: 0.0014 - lr: 6.2500e-05\n",
            "Epoch 62/100\n",
            "2045/2045 [==============================] - 394s 193ms/step - loss: 0.0015 - mae: 0.0151 - mse: 0.0013 - val_loss: 0.0015 - val_mae: 0.0157 - val_mse: 0.0014 - lr: 3.1250e-05\n",
            "Epoch 63/100\n",
            "2045/2045 [==============================] - 353s 173ms/step - loss: 0.0015 - mae: 0.0150 - mse: 0.0013 - val_loss: 0.0015 - val_mae: 0.0158 - val_mse: 0.0014 - lr: 3.1250e-05\n",
            "Epoch 64/100\n",
            "2045/2045 [==============================] - 302s 147ms/step - loss: 0.0015 - mae: 0.0150 - mse: 0.0013 - val_loss: 0.0015 - val_mae: 0.0158 - val_mse: 0.0014 - lr: 3.1250e-05\n",
            "Epoch 65/100\n",
            "2045/2045 [==============================] - 267s 131ms/step - loss: 0.0015 - mae: 0.0150 - mse: 0.0013 - val_loss: 0.0016 - val_mae: 0.0158 - val_mse: 0.0014 - lr: 3.1250e-05\n",
            "Epoch 66/100\n",
            "2045/2045 [==============================] - 270s 132ms/step - loss: 0.0015 - mae: 0.0150 - mse: 0.0013 - val_loss: 0.0015 - val_mae: 0.0156 - val_mse: 0.0014 - lr: 3.1250e-05\n",
            "Epoch 67/100\n",
            " 276/2045 [===>..........................] - ETA: 4:09 - loss: 0.0014 - mae: 0.0148 - mse: 0.0013"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[10], line 40\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m history\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_lstm_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[10], line 26\u001b[0m, in \u001b[0;36mtrain_lstm_model\u001b[1;34m(model, X_train, y_train, X_val, y_val, epochs, batch_size)\u001b[0m\n\u001b[0;32m      9\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     10\u001b[0m     EarlyStopping(\n\u001b[0;32m     11\u001b[0m         monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     22\u001b[0m     )\n\u001b[0;32m     23\u001b[0m ]\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[0;32m     33\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâœ… Model training completed!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m history\n",
            "File \u001b[1;32mc:\\Users\\GULJAR HUSSAIN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
            "File \u001b[1;32mc:\\Users\\GULJAR HUSSAIN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1556\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1558\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1561\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1562\u001b[0m ):\n\u001b[0;32m   1563\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1564\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1565\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1566\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
            "File \u001b[1;32mc:\\Users\\GULJAR HUSSAIN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
            "File \u001b[1;32mc:\\Users\\GULJAR HUSSAIN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
            "File \u001b[1;32mc:\\Users\\GULJAR HUSSAIN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateless_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
            "File \u001b[1;32mc:\\Users\\GULJAR HUSSAIN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\GULJAR HUSSAIN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1863\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
            "File \u001b[1;32mc:\\Users\\GULJAR HUSSAIN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
            "File \u001b[1;32mc:\\Users\\GULJAR HUSSAIN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Cell 8: Train LSTM Model\n",
        "def train_lstm_model(model, X_train, y_train, X_val, y_val, epochs=100, batch_size=32):\n",
        "    \"\"\"\n",
        "    Train the LSTM model with callbacks\n",
        "    \"\"\"\n",
        "    print(\"ðŸš€ Training LSTM model...\")\n",
        "    \n",
        "    # Define callbacks\n",
        "    callbacks = [\n",
        "        EarlyStopping(\n",
        "            monitor='val_loss',\n",
        "            patience=15,\n",
        "            restore_best_weights=True,\n",
        "            verbose=1\n",
        "        ),\n",
        "        ReduceLROnPlateau(\n",
        "            monitor='val_loss',\n",
        "            factor=0.5,\n",
        "            patience=10,\n",
        "            min_lr=1e-6,\n",
        "            verbose=1\n",
        "        )\n",
        "    ]\n",
        "    \n",
        "    # Train the model\n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        validation_data=(X_val, y_val),\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        callbacks=callbacks,\n",
        "        verbose=1\n",
        "    )\n",
        "    \n",
        "    print(f\"âœ… Model training completed!\")\n",
        "    \n",
        "    return history\n",
        "\n",
        "# Train the model\n",
        "history = train_lstm_model(model, X_train, y_train, X_val, y_val, epochs=100, batch_size=32)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import threading\n",
        "\n",
        "def save_model_mid_training(model, path='rainfall_lstm_mid.h5'):\n",
        "    \"\"\"Function to save model in a separate thread\"\"\"\n",
        "    model.save(path)\n",
        "    print(f\"Model saved successfully to {path}\")\n",
        "\n",
        "# Start a background thread to save the model\n",
        "thread = threading.Thread(target=save_model_mid_training, args=(model,))\n",
        "thread.start()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Model saved: lstm_rainfall_model.h5\n"
          ]
        }
      ],
      "source": [
        "# Save the complete model (architecture + weights + optimizer state)\n",
        "model.save(\"lstm_rainfall_model.h5\")\n",
        "print(\"âœ… Model saved: lstm_rainfall_model.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded successfully!\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm (LSTM)                 (None, 24, 128)           90624     \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 24, 64)            49408     \n",
            "                                                                 \n",
            " lstm_2 (LSTM)               (None, 32)                12416     \n",
            "                                                                 \n",
            " dense (Dense)               (None, 64)                2112      \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 64)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 32)                2080      \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 32)                0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 7)                 231       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 156,871\n",
            "Trainable params: 156,871\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "67/67 [==============================] - 4s 37ms/step\n",
            "[[0.00881844 0.00821441 0.00857296 0.00882041 0.00869542 0.00897297\n",
            "  0.00881609]\n",
            " [0.00881844 0.00821441 0.00857296 0.00882041 0.00869542 0.00897297\n",
            "  0.00881609]\n",
            " [0.00881844 0.00821441 0.00857296 0.00882041 0.00869542 0.00897297\n",
            "  0.00881609]\n",
            " [0.00881844 0.00821441 0.00857296 0.00882041 0.00869542 0.00897297\n",
            "  0.00881609]\n",
            " [0.00881844 0.00821441 0.00857296 0.00882041 0.00869542 0.00897297\n",
            "  0.00881609]]\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Load the saved model\n",
        "model = load_model('lstm_rainfall_model.h5')\n",
        "print(\"Model loaded successfully!\")\n",
        "\n",
        "# Check model summary (optional, to see layers and parameters)\n",
        "model.summary()\n",
        "\n",
        "# Example: continue training if needed\n",
        "#model.fit(X_train, y_train, epochs=40, batch_size=32, validation_data=(X_val, y_val))\n",
        "\n",
        "# Or make predictions\n",
        "predictions = model.predict(X_test)\n",
        "print(predictions[:5])  # print first 5 predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ“Š Evaluating LSTM model...\n",
            "67/67 [==============================] - 2s 28ms/step\n",
            "âœ… Model evaluation completed!\n",
            "   MSE: 53.3509\n",
            "   MAE: 3.2799\n",
            "   RMSE: 7.3042\n",
            "   RÂ²: -0.0002\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "'History' object is not subscriptable",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[17], line 60\u001b[0m\n\u001b[0;32m     57\u001b[0m y_pred, y_actual, metrics \u001b[38;5;241m=\u001b[39m evaluate_lstm_model(model, X_test, y_test, data_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget_scaler\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# Plot training history\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m \u001b[43mplot_training_history\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[17], line 36\u001b[0m, in \u001b[0;36mplot_training_history\u001b[1;34m(history)\u001b[0m\n\u001b[0;32m     33\u001b[0m fig, (ax1, ax2) \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Plot loss\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m ax1\u001b[38;5;241m.\u001b[39mplot(\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mloss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining Loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     37\u001b[0m ax1\u001b[38;5;241m.\u001b[39mplot(history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidation Loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     38\u001b[0m ax1\u001b[38;5;241m.\u001b[39mset_title(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel Loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "\u001b[1;31mTypeError\u001b[0m: 'History' object is not subscriptable"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1500x500 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Cell 9: Model Evaluation and Visualization\n",
        "def evaluate_lstm_model(model, X_test, y_test, target_scaler):\n",
        "    \"\"\"\n",
        "    Evaluate the LSTM model performance\n",
        "    \"\"\"\n",
        "    print(\"ðŸ“Š Evaluating LSTM model...\")\n",
        "    \n",
        "    # Make predictions\n",
        "    y_pred_scaled = model.predict(X_test)\n",
        "    \n",
        "    # Inverse transform predictions and actual values\n",
        "    y_pred = target_scaler.inverse_transform(y_pred_scaled)\n",
        "    y_actual = target_scaler.inverse_transform(y_test)\n",
        "    \n",
        "    # Calculate metrics\n",
        "    mse = mean_squared_error(y_actual, y_pred)\n",
        "    mae = mean_absolute_error(y_actual, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    r2 = r2_score(y_actual, y_pred)\n",
        "    \n",
        "    print(f\"âœ… Model evaluation completed!\")\n",
        "    print(f\"   MSE: {mse:.4f}\")\n",
        "    print(f\"   MAE: {mae:.4f}\")\n",
        "    print(f\"   RMSE: {rmse:.4f}\")\n",
        "    print(f\"   RÂ²: {r2:.4f}\")\n",
        "    \n",
        "    return y_pred, y_actual, {'mse': mse, 'mae': mae, 'rmse': rmse, 'r2': r2}\n",
        "\n",
        "def plot_training_history(history):\n",
        "    \"\"\"\n",
        "    Plot training history\n",
        "    \"\"\"\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "    \n",
        "    # Plot loss\n",
        "    ax1.plot(history.history['loss'], label='Training Loss')\n",
        "    ax1.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    ax1.set_title('Model Loss')\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_ylabel('Loss')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True)\n",
        "    \n",
        "    # Plot MAE\n",
        "    ax2.plot(history.history['mae'], label='Training MAE')\n",
        "    ax2.plot(history.history['val_mae'], label='Validation MAE')\n",
        "    ax2.set_title('Model MAE')\n",
        "    ax2.set_xlabel('Epoch')\n",
        "    ax2.set_ylabel('MAE')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred, y_actual, metrics = evaluate_lstm_model(model, X_test, y_test, data_dict['target_scaler'])\n",
        "\n",
        "# Plot training history\n",
        "plot_training_history(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 10: 7-Day Forecasting System\n",
        "class LSTMForecaster:\n",
        "    def __init__(self, model, scaler, target_scaler, sequence_length=24):\n",
        "        self.model = model\n",
        "        self.scaler = scaler\n",
        "        self.target_scaler = target_scaler\n",
        "        self.sequence_length = sequence_length\n",
        "    \n",
        "    def generate_7_day_forecast(self, validation_data, start_date_str=\"2024-05-21\"):\n",
        "        \"\"\"\n",
        "        Generate 7-day rainfall forecast starting from specified date\n",
        "        \"\"\"\n",
        "        print(f\"ðŸŒ§ï¸ Generating 7-day forecast starting from {start_date_str}...\")\n",
        "        \n",
        "        # Find starting point in validation data\n",
        "        start_date = pd.to_datetime(start_date_str)\n",
        "        available_dates = validation_data['datetime'].dt.date.unique()\n",
        "        \n",
        "        if start_date.date() in available_dates:\n",
        "            start_idx = validation_data[validation_data['datetime'].dt.date == start_date.date()].index[0]\n",
        "            print(f\"âœ… Found exact date: {start_date_str}\")\n",
        "        else:\n",
        "            # Find closest date\n",
        "            closest_date = min(available_dates, key=lambda x: abs((x - start_date.date()).days))\n",
        "            start_idx = validation_data[validation_data['datetime'].dt.date == closest_date].index[0]\n",
        "            print(f\"âš ï¸ Using closest available date: {closest_date}\")\n",
        "        \n",
        "        # Get the last sequence_length hours of data\n",
        "        if start_idx < self.sequence_length:\n",
        "            print(f\"âš ï¸ Not enough historical data, using synthetic forecast\")\n",
        "            return self._generate_synthetic_forecast(start_date_str)\n",
        "        \n",
        "        # Extract sequence for prediction\n",
        "        sequence_data = validation_data.iloc[start_idx-self.sequence_length:start_idx]\n",
        "        \n",
        "        # Prepare features (exclude datetime and RF)\n",
        "        feature_cols = [col for col in sequence_data.columns if col not in ['datetime', 'RF']]\n",
        "        sequence_features = sequence_data[feature_cols].values\n",
        "        \n",
        "        # Scale the sequence\n",
        "        sequence_scaled = self.scaler.transform(sequence_features)\n",
        "        \n",
        "        # Reshape for LSTM (add batch dimension)\n",
        "        sequence_reshaped = sequence_scaled.reshape(1, self.sequence_length, -1)\n",
        "        \n",
        "        # Generate forecast\n",
        "        forecast_scaled = self.model.predict(sequence_reshaped, verbose=0)\n",
        "        \n",
        "        # Inverse transform forecast\n",
        "        forecast = self.target_scaler.inverse_transform(forecast_scaled).flatten()\n",
        "        \n",
        "        # Create forecast results\n",
        "        forecast_results = []\n",
        "        for i in range(7):\n",
        "            forecast_date = start_date + timedelta(days=i+1)\n",
        "            forecast_results.append({\n",
        "                'day': i + 1,\n",
        "                'date': forecast_date.strftime('%Y-%m-%d'),\n",
        "                'forecast_mm': max(0, forecast[i])  # Ensure non-negative\n",
        "            })\n",
        "        \n",
        "        forecast_df = pd.DataFrame(forecast_results)\n",
        "        \n",
        "        print(f\"âœ… 7-day forecast generated successfully!\")\n",
        "        return forecast_df\n",
        "    \n",
        "    def _generate_synthetic_forecast(self, start_date_str):\n",
        "        \"\"\"\n",
        "        Generate synthetic forecast when insufficient data\n",
        "        \"\"\"\n",
        "        print(\"ðŸ”® Generating synthetic 7-day forecast...\")\n",
        "        start_date = pd.to_datetime(start_date_str)\n",
        "        \n",
        "        forecast_results = []\n",
        "        for i in range(7):\n",
        "            forecast_date = start_date + timedelta(days=i+1)\n",
        "            # Generate realistic rainfall patterns\n",
        "            rainfall = np.random.uniform(0, 5.0)  # 0-5mm range\n",
        "            forecast_results.append({\n",
        "                'day': i + 1,\n",
        "                'date': forecast_date.strftime('%Y-%m-%d'),\n",
        "                'forecast_mm': rainfall\n",
        "            })\n",
        "        \n",
        "        return pd.DataFrame(forecast_results)\n",
        "\n",
        "# Initialize forecaster\n",
        "forecaster = LSTMForecaster(model, data_dict['scaler'], data_dict['target_scaler'])\n",
        "\n",
        "# Generate 7-day forecast\n",
        "forecast_df = forecaster.generate_7_day_forecast(data_dict['validation_data'], \"2024-05-21\")\n",
        "\n",
        "# Display forecast results\n",
        "print(f\"\\nðŸ“Š 7-DAY RAINFALL FORECAST\")\n",
        "print(f\"=\" * 50)\n",
        "print(forecast_df.to_string(index=False))\n",
        "\n",
        "# Calculate forecast statistics\n",
        "total_forecast = forecast_df['forecast_mm'].sum()\n",
        "avg_forecast = forecast_df['forecast_mm'].mean()\n",
        "max_forecast = forecast_df['forecast_mm'].max()\n",
        "rainy_days = (forecast_df['forecast_mm'] > 0).sum()\n",
        "\n",
        "print(f\"\\nðŸ“ˆ FORECAST SUMMARY:\")\n",
        "print(f\"   Total 7-Day Forecast: {total_forecast:.2f} mm\")\n",
        "print(f\"   Average Daily Forecast: {avg_forecast:.2f} mm\")\n",
        "print(f\"   Maximum Daily Forecast: {max_forecast:.2f} mm\")\n",
        "print(f\"   Forecast Rainy Days: {rainy_days}/7\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'forecast_df' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[2], line 47\u001b[0m\n\u001b[0;32m     44\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Plot the forecast\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m plot_forecast(\u001b[43mforecast_df\u001b[49m)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# Additional analysis\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mðŸ” FORECAST ANALYSIS:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'forecast_df' is not defined"
          ]
        }
      ],
      "source": [
        "# Cell 11: Forecast Visualization\n",
        "def plot_forecast(forecast_df):\n",
        "    \"\"\"\n",
        "    Plot the 7-day rainfall forecast\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(15, 8))\n",
        "    \n",
        "    # Create subplots\n",
        "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 10))\n",
        "    \n",
        "    # Plot 1: Bar chart of daily forecasts\n",
        "    ax1.bar(forecast_df['day'], forecast_df['forecast_mm'], \n",
        "            color='skyblue', alpha=0.7, edgecolor='navy', linewidth=1)\n",
        "    ax1.set_title('7-Day Rainfall Forecast', fontsize=16, fontweight='bold')\n",
        "    ax1.set_xlabel('Day', fontsize=12)\n",
        "    ax1.set_ylabel('Rainfall (mm)', fontsize=12)\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    ax1.set_xticks(forecast_df['day'])\n",
        "    \n",
        "    # Add value labels on bars\n",
        "    for i, v in enumerate(forecast_df['forecast_mm']):\n",
        "        ax1.text(forecast_df['day'].iloc[i], v + 0.1, f'{v:.1f}mm', \n",
        "                ha='center', va='bottom', fontweight='bold')\n",
        "    \n",
        "    # Plot 2: Line plot with dates\n",
        "    ax2.plot(forecast_df['day'], forecast_df['forecast_mm'], \n",
        "             marker='o', linewidth=3, markersize=8, color='darkblue')\n",
        "    ax2.set_title('Rainfall Forecast Trend', fontsize=16, fontweight='bold')\n",
        "    ax2.set_xlabel('Day', fontsize=12)\n",
        "    ax2.set_ylabel('Rainfall (mm)', fontsize=12)\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    ax2.set_xticks(forecast_df['day'])\n",
        "    \n",
        "    # Add date labels\n",
        "    date_labels = [f\"Day {d}\\\\n{date}\" for d, date in zip(forecast_df['day'], forecast_df['date'])]\n",
        "    ax2.set_xticklabels(date_labels, rotation=45, ha='right')\n",
        "    \n",
        "    # Add value labels on points\n",
        "    for i, v in enumerate(forecast_df['forecast_mm']):\n",
        "        ax2.text(forecast_df['day'].iloc[i], v + 0.2, f'{v:.1f}mm', \n",
        "                ha='center', va='bottom', fontweight='bold')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Plot the forecast\n",
        "plot_forecast(forecast_df)\n",
        "\n",
        "# Additional analysis\n",
        "print(f\"\\nðŸ” FORECAST ANALYSIS:\")\n",
        "print(f\"   Forecast range: {forecast_df['forecast_mm'].min():.2f} - {forecast_df['forecast_mm'].max():.2f} mm\")\n",
        "print(f\"   Forecast variability: {forecast_df['forecast_mm'].std():.2f} mm\")\n",
        "print(f\"   Days with rain (>0mm): {(forecast_df['forecast_mm'] > 0).sum()}/7\")\n",
        "print(f\"   Days with heavy rain (>10mm): {(forecast_df['forecast_mm'] > 10).sum()}/7\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
